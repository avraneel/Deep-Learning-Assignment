{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhNFmeWwX58Y"
      },
      "source": [
        "# Creating K-fold Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OYaGGfwnN4E-",
        "outputId": "8ff5ce50-11d8-4695-d302-8c016ce7db36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNWRECVXkFz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "input_dataset_dir='./drive/MyDrive/Datasets/original_dataset/'\n",
        "output_dataset_dir='./drive/MyDrive/Datasets/kfold_dataset/'\n",
        "\n",
        "\n",
        "def create_splits(k_folds=5):\n",
        "    class_names = os.listdir(input_dataset_dir)\n",
        "\n",
        "    # creating output directory\n",
        "    os.makedirs(output_dataset_dir, exist_ok=True)\n",
        "\n",
        "    # Perform K-Fold cross-validation\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # Iterating over each class\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(input_dataset_dir, class_name)\n",
        "\n",
        "        # Listing all images of that class\n",
        "        all_images = os.listdir(class_path)\n",
        "        random.shuffle(all_images)\n",
        "        # Getting labels for each image\n",
        "        labels = [class_name] * len(all_images)\n",
        "\n",
        "        # Split the dataset into train and test sets using k-fold\n",
        "        for fold, (_, _) in enumerate(kf.split(all_images, labels)):\n",
        "            # Create fold directories\n",
        "            train_fold_dir = os.path.join(output_dataset_dir, f'fold_{fold + 1}', 'train', class_name)\n",
        "            val_fold_dir = os.path.join(output_dataset_dir, f'fold_{fold + 1}', 'validation', class_name)\n",
        "            test_fold_dir = os.path.join(output_dataset_dir, f'fold_{fold + 1}', 'test', class_name)\n",
        "\n",
        "            os.makedirs(train_fold_dir, exist_ok=True)\n",
        "            os.makedirs(val_fold_dir, exist_ok=True)\n",
        "            os.makedirs(test_fold_dir, exist_ok=True)\n",
        "\n",
        "            #print(f'Directories created...')\n",
        "\n",
        "            #print(f'Doing train-test-split...')\n",
        "            train_images, val_test_images = train_test_split(all_images, test_size=0.4, stratify=labels,\n",
        "                                                             random_state=42)\n",
        "            #print(f'Doing validation test split...')\n",
        "            val_images, test_images = train_test_split(val_test_images, test_size=0.5,\n",
        "                                                       stratify=[class_name] * len(val_test_images), random_state=42)\n",
        "\n",
        "            #print('Copying training images...')\n",
        "            for image in train_images:\n",
        "                shutil.copy(os.path.join(class_path, image), os.path.join(train_fold_dir, image))\n",
        "\n",
        "            #print('Copying validation images...')\n",
        "            for image in val_images:\n",
        "                shutil.copy(os.path.join(class_path, image), os.path.join(val_fold_dir, image))\n",
        "\n",
        "            #print('Copying test images...')\n",
        "            for image in test_images:\n",
        "                shutil.copy(os.path.join(class_path, image), os.path.join(test_fold_dir, image))\n",
        "\n",
        "\n",
        "create_splits()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amn-A7nrbXYY"
      },
      "source": [
        "# Creating Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPvldMoj-Uyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc087ab-2e4e-4e77-df3b-fff701e885cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 112, 112, 32)      864       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 112, 112, 32)      96        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 55, 55, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 55, 55, 64)        18432     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 55, 55, 64)        192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 55, 55, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 55, 55, 96)        55296     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 55, 55, 96)        288       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 55, 55, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 27, 27, 96)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 27, 27, 128)       110592    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 27, 27, 128)       384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 27, 27, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 27, 27, 256)       294912    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 27, 27, 256)       768       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 27, 27, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 384)       884736    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 13, 13, 384)       1152      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 13, 13, 384)       0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 384)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 384)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                24640     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1392677 (5.31 MB)\n",
            "Trainable params: 1390757 (5.31 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Activation, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class CNNModel:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.model = CNNModel.build_model(input_shape, num_classes)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model(input_shape, num_classes):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, (3, 3), padding='same', strides=(1, 1), input_shape=input_shape, use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', strides=(1, 1), input_shape=(55, 55, 32), use_bias=False))\n",
        "        # model.add(Conv2D(64, (3, 3), padding='same', strides=(1, 1), use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Conv2D(96, (3, 3), padding='same', strides=(1, 1), input_shape=(55, 55, 64), use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same', strides=(1, 1), input_shape=(27, 27, 96), use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same', strides=(1, 1), input_shape=(27, 27, 128), use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(384, (3, 3), padding='same', strides=(1, 1), input_shape=(13, 13, 256), use_bias=False))\n",
        "        model.add(BatchNormalization(scale=False))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "\n",
        "        model.add(Dense(64, activation='softmax'))\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        model.summary()\n",
        "        # keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def train(self, train_dataset, validation_dataset, epochs, batch_size, cp_callback):\n",
        "        history = self.model.fit(train_dataset,\n",
        "                                 epochs=epochs,\n",
        "                                 batch_size=batch_size,\n",
        "                                 validation_data=validation_dataset,\n",
        "                                 callbacks=[cp_callback])\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, test_dataset):\n",
        "        return self.model.evaluate(test_dataset)\n",
        "\n",
        "    def predict(self, test_dataset):\n",
        "      return self.model.predict(test_dataset)\n",
        "\n",
        "    def save(self, path):\n",
        "      return self.model.save(path)\n",
        "\n",
        "\n",
        "image_shape=(112,112,3)\n",
        "num_classes=5\n",
        "model = CNNModel(input_shape=image_shape, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM3tikhjetNy"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkRJESaFesrl",
        "outputId": "a3766c00-3e08-4786-b8a3-c2d3ef253663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 begins...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 112, 112, 32)      864       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 112, 112, 32)      96        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 55, 55, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 55, 55, 64)        18432     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 55, 55, 64)        192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 55, 55, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 55, 55, 96)        55296     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 55, 55, 96)        288       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 55, 55, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 27, 27, 96)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 27, 27, 128)       110592    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 27, 27, 128)       384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 27, 27, 128)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 27, 27, 256)       294912    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 27, 27, 256)       768       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 27, 27, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 256)       589824    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 13, 13, 256)       768       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 13, 13, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 256)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1089189 (4.15 MB)\n",
            "Trainable params: 1087525 (4.15 MB)\n",
            "Non-trainable params: 1664 (6.50 KB)\n",
            "_________________________________________________________________\n",
            "Found 2427 files belonging to 5 classes.\n",
            "Found 810 files belonging to 5 classes.\n",
            "Found 812 files belonging to 5 classes.\n",
            "Epoch 1/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.4284 - accuracy: 0.6535\n",
            "Epoch 1: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 364s 5s/step - loss: 1.4284 - accuracy: 0.6535 - val_loss: 1.5205 - val_accuracy: 0.4864\n",
            "Epoch 2/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.3474 - accuracy: 0.7256\n",
            "Epoch 2: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 365s 5s/step - loss: 1.3474 - accuracy: 0.7256 - val_loss: 1.4274 - val_accuracy: 0.5593\n",
            "Epoch 3/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.2850 - accuracy: 0.7466\n",
            "Epoch 3: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 355s 5s/step - loss: 1.2850 - accuracy: 0.7466 - val_loss: 1.4892 - val_accuracy: 0.4123\n",
            "Epoch 4/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.2099 - accuracy: 0.7713\n",
            "Epoch 4: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 339s 4s/step - loss: 1.2099 - accuracy: 0.7713 - val_loss: 1.2405 - val_accuracy: 0.6901\n",
            "Epoch 5/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.1392 - accuracy: 0.7927\n",
            "Epoch 5: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 345s 5s/step - loss: 1.1392 - accuracy: 0.7927 - val_loss: 1.2613 - val_accuracy: 0.6420\n",
            "Epoch 6/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.0679 - accuracy: 0.8162\n",
            "Epoch 6: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 339s 4s/step - loss: 1.0679 - accuracy: 0.8162 - val_loss: 1.0639 - val_accuracy: 0.7951\n",
            "Epoch 7/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.0171 - accuracy: 0.8183\n",
            "Epoch 7: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 337s 4s/step - loss: 1.0171 - accuracy: 0.8183 - val_loss: 1.1960 - val_accuracy: 0.6222\n",
            "Epoch 8/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.9706 - accuracy: 0.8154\n",
            "Epoch 8: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 338s 4s/step - loss: 0.9706 - accuracy: 0.8154 - val_loss: 1.1716 - val_accuracy: 0.6346\n",
            "Epoch 9/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.9314 - accuracy: 0.8179\n",
            "Epoch 9: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 330s 4s/step - loss: 0.9314 - accuracy: 0.8179 - val_loss: 1.3362 - val_accuracy: 0.5074\n",
            "Epoch 10/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.8803 - accuracy: 0.8307\n",
            "Epoch 10: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 335s 4s/step - loss: 0.8803 - accuracy: 0.8307 - val_loss: 1.1125 - val_accuracy: 0.6519\n",
            "Epoch 11/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.8451\n",
            "Epoch 11: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 327s 4s/step - loss: 0.8234 - accuracy: 0.8451 - val_loss: 0.8467 - val_accuracy: 0.8210\n",
            "Epoch 12/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.8488\n",
            "Epoch 12: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 325s 4s/step - loss: 0.7912 - accuracy: 0.8488 - val_loss: 1.0025 - val_accuracy: 0.7037\n",
            "Epoch 13/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.7555 - accuracy: 0.8591\n",
            "Epoch 13: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 323s 4s/step - loss: 0.7555 - accuracy: 0.8591 - val_loss: 0.7100 - val_accuracy: 0.8778\n",
            "Epoch 14/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.8698\n",
            "Epoch 14: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 326s 4s/step - loss: 0.7101 - accuracy: 0.8698 - val_loss: 0.8681 - val_accuracy: 0.7593\n",
            "Epoch 15/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.8756\n",
            "Epoch 15: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 304s 4s/step - loss: 0.6706 - accuracy: 0.8756 - val_loss: 0.8478 - val_accuracy: 0.7642\n",
            "Epoch 16/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.8789\n",
            "Epoch 16: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 325s 4s/step - loss: 0.6474 - accuracy: 0.8789 - val_loss: 1.0780 - val_accuracy: 0.6469\n",
            "Epoch 17/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.8949\n",
            "Epoch 17: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 315s 4s/step - loss: 0.5954 - accuracy: 0.8949 - val_loss: 0.6395 - val_accuracy: 0.8593\n",
            "Epoch 18/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.8888\n",
            "Epoch 18: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 324s 4s/step - loss: 0.5888 - accuracy: 0.8888 - val_loss: 0.7667 - val_accuracy: 0.7765\n",
            "Epoch 19/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.8867\n",
            "Epoch 19: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 324s 4s/step - loss: 0.5746 - accuracy: 0.8867 - val_loss: 0.5640 - val_accuracy: 0.8889\n",
            "Epoch 20/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8797\n",
            "Epoch 20: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 325s 4s/step - loss: 0.5739 - accuracy: 0.8797 - val_loss: 0.6275 - val_accuracy: 0.8494\n",
            "Epoch 21/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.8850\n",
            "Epoch 21: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 322s 4s/step - loss: 0.5311 - accuracy: 0.8850 - val_loss: 0.5582 - val_accuracy: 0.8667\n",
            "Epoch 22/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.8916\n",
            "Epoch 22: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 323s 4s/step - loss: 0.5171 - accuracy: 0.8916 - val_loss: 0.6075 - val_accuracy: 0.8444\n",
            "Epoch 23/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.9061\n",
            "Epoch 23: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 326s 4s/step - loss: 0.4718 - accuracy: 0.9061 - val_loss: 0.5063 - val_accuracy: 0.8815\n",
            "Epoch 24/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.4683 - accuracy: 0.9023\n",
            "Epoch 24: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 323s 4s/step - loss: 0.4683 - accuracy: 0.9023 - val_loss: 0.5357 - val_accuracy: 0.8704\n",
            "Epoch 25/40\n",
            "76/76 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.9032\n",
            "Epoch 25: saving model to ./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_40_fold_1_20231126154000.ckpt\n",
            "76/76 [==============================] - 310s 4s/step - loss: 0.4600 - accuracy: 0.9032 - val_loss: 0.5176 - val_accuracy: 0.8728\n",
            "Epoch 26/40\n",
            "44/76 [================>.............] - ETA: 1:57 - loss: 0.4273 - accuracy: 0.9176"
          ]
        }
      ],
      "source": [
        "#from model import CNNModel\n",
        "import keras.utils\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime, os\n",
        "\n",
        "# Parameters\n",
        "k_folds=5\n",
        "batch_size=32\n",
        "image_size= (112,112)\n",
        "image_shape=(112,112,3)\n",
        "num_classes=5\n",
        "epochs=40\n",
        "\n",
        "for fold in range(k_folds):\n",
        "    print(f'Fold {fold+1} begins...')\n",
        "    model = CNNModel(input_shape=image_shape, num_classes=num_classes)\n",
        "    # Getting paths for each fold\n",
        "    train_dir = f'./drive/MyDrive/Datasets/kfold_dataset/fold_{fold+1}/train'\n",
        "    val_dir = f'./drive/MyDrive/Datasets/kfold_dataset/fold_{fold+1}/validation'\n",
        "    test_dir = f'./drive/MyDrive/Datasets/kfold_dataset/fold_{fold+1}/test'\n",
        "\n",
        "    # Loading train, validation and test datasets for each fold\n",
        "    train_dataset = keras.utils.image_dataset_from_directory(\n",
        "        directory=train_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    val_dataset = keras.utils.image_dataset_from_directory(\n",
        "        directory=val_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    test_dataset = keras.utils.image_dataset_from_directory(\n",
        "        directory=test_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=image_size\n",
        "    )\n",
        "    #----------------------------------------------------------\n",
        "\n",
        "    unique = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "    # Saving checkpoints\n",
        "    checkpoint_path=f'./drive/MyDrive/Datasets/models/saved_weights/cnn_model_epoch_{epochs}_fold_{fold+1}_{unique}.ckpt'\n",
        "    checkpoint_dir =os.path.dirname(checkpoint_path)\n",
        "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "    # Training\n",
        "    history = model.train(train_dataset, val_dataset, epochs, batch_size, cp_callback)\n",
        "\n",
        "    # Plotting accuracy\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'Accuracy graph for fold {fold+1}')\n",
        "    plt.plot()\n",
        "    #plt.show()\n",
        "    plt.savefig(f'./drive/MyDrive/Datasets/results/accuracy_epoch_{epochs}_fold_{fold+1}_{unique}.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting loss\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='validation loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'Loss graph for fold {fold+1}')\n",
        "    plt.plot()\n",
        "    #plt.show()\n",
        "    plt.savefig(f'./drive/MyDrive/Datasets/results/loss_epoch_{epochs}_fold_{fold+1}_{unique}.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Prediction\n",
        "    loss, accuracy = model.evaluate(test_dataset)\n",
        "    print(f'Test Loss: {loss}')\n",
        "    print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "    # Saving entire model\n",
        "    model.save(f'./drive/MyDrive/Datasets/models/saved_models/cnn_model_epoch_{epochs}_fold_{fold+1}_{unique}.keras')\n",
        "\n",
        "    print(f'Fold {fold+1} ends...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqbPON0IPtMy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "k_folds = 5\n",
        "p1 = './models/saved_models/'\n",
        "uid = '20231126'\n",
        "batch_size = 32\n",
        "image_size = (112, 112, 3)\n",
        "\n",
        "for fold in k_folds:\n",
        "    test_dir = f'./kfold_dataset/fold_{fold + 1}/test'\n",
        "\n",
        "    test_dataset = keras.utils.image_dataset_from_directory(\n",
        "        directory=test_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=batch_size,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    true_labels = []\n",
        "    for features, labels in test_dataset:\n",
        "        true_labels.extend(tf.argmax(labels, axis=1).numpy())\n",
        "\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    model = keras.models.load_model(os.path.join(p1, f'cnn_model_epoch_40_fold_{fold}_{uid}.keras'))\n",
        "\n",
        "    print(f'Printing results for fold{fold + 1}:')\n",
        "    print('----------------------------------')\n",
        "\n",
        "    y_pred = model.predict(test_dataset)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    accuracy_sk = accuracy_score(true_labels, y_pred_classes)\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "    print(f'Accuracy from sklearn accuracy score: {accuracy_sk}')\n",
        "    print(f'Accuracy from model.evaluate(): {accuracy}')\n",
        "\n",
        "    correct_labels = np.where(y_pred_classes == true_labels)[0]\n",
        "    wrong_labels = np.where(y_pred_classes != true_labels)[0]\n",
        "\n",
        "    print(f'Correct Labels: {len(correct_labels)}')\n",
        "    print(f'Wrong Labels: {len(wrong_labels)}')\n",
        "\n",
        "    cm = confusion_matrix(true_labels, y_pred_classes)\n",
        "\n",
        "    class_report = classification_report(true_labels, y_pred_classes)\n",
        "    print('Classification Report:\\n', class_report)\n",
        "\n",
        "    print(f'Fold {fold+1} evaluation ends.')\n",
        "    print('------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}